import { Callout } from 'nextra/components'

# Lilypad

Lilypad is an open-source prompt engineering framework built on these principles:

- Prompt engineering is an optimization process, which requires...
- Versioning and tracing of every prompt for...
- Building a data flywheel, so you can...
- Properly evaluate and iterate on your prompts and...
- Ship confidently

The primary goal of Lilypad is to automate as much of this process as possible so you can focus on building your application and making it good.

<Callout type="info" emoji="ℹ️">
    **Lilypad is currently in a closed beta.**

    You can install and use the library locally today, but we are working with a few select teams to ensure that the library is ready for broader, production use.

    This means that things like user interface, database schemas, etc. are still subject to change.
    
    If you're interested in joining the beta, [join our community](https://join.slack.com/t/mirascope-community/shared_invite/zt-2ilqhvmki-FB6LWluInUCkkjYD3oSjNA) and send a message in the #lilypad channel about your team and why you think you would be a good fit.
</Callout>

## 30 Second Quickstart

Install Lilypad, specifying the provider(s) you intend to use, and set your API key:

```bash
pip install "python-lilypad[openai]"
export OPENAI_API_KEY=XXXXX
```

Spin up the local Lilypad server:

```bash
lilypad local
```

Create your first traced prompt function to answer an important question:

```python
import lilypad
from openai import OpenAI

lilypad.configure()


@lilypad.trace()
def answer_question(question: str) -> str:
    client = OpenAI()
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": question}],
    )
    return str(completion.choices[0].message.content)


answer = answer_question("What is the meaning of life?")
print(answer)
```

Navigate to the Lilypad UI at `http://localhost:8000` to see the version and trace of your prompt.

Next, try updating your function's prompt:

```python
@lilypad.trace()
def answer_question(question: str) -> str:
    client = OpenAI()
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": f"Answer this question: {question}"}],
    )
    return str(completion.choices[0].message.content)
```

Run the function again and see the new version and trace in the UI.

And that's it. Now every call to the `answer_question` function will be versioned and traced automatically.
